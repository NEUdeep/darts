darts论文链接：https://arxiv.org/pdf/1806.09055.pdf
darts源码链接：https://github.com/quark0/darts

------

![img](https://img-blog.csdnimg.cn/20200416184516931.png)

###### `search部分`

```python
'''
train_search.py
#数据准备（cifar10）。
搜索时，从cifar10的训练集中按照1:1重新划分训练集和验证集
'''
train_transform, valid_transform = utils._data_transforms_cifar10(args)
train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)

#论文中 args.train_portion 取0.5
num_train = len(train_data)
indices = list(range(num_train))
split = int(np.floor(args.train_portion * num_train))

train_queue = torch.utils.data.DataLoader(
      train_data, batch_size=args.batch_size,
      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),
      pin_memory=True, num_workers=2)
valid_queue = torch.utils.data.DataLoader(
      train_data, batch_size=args.batch_size,
      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),
      pin_memory=True, num_workers=2)
      
12345678910111213141516171819202122
'''
train_search.py
搜索网络
损失函数：交叉熵
优化器：带动量的SGD
学习率调整策略：余弦退火调整学习率 CosineAnnealingLR
'''
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(
      model.parameters(),
      args.learning_rate,
      momentum=args.momentum,
      weight_decay=args.weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, float(args.epochs), eta_min=args.learning_rate_min)

123456789101112131415
'''
train_search.py
构建搜索网络
构建Architect优化
'''
# in model_search.py
model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)
# in architect.py
architect = Architect(model, args)

12345678910
'''
model_search.py
论文中
# C	：16
# num_classes ：2
# criterion
# layers：8
'''
class Network(nn.Module):
	def __init__(self, C, num_classes, layers, criterion, steps=4, multiplier=4, stem_multiplier=3):
	    super(Network, self).__init__()
	    self._C = C
	    self._num_classes = num_classes
	    self._layers = layers
	    self._criterion = criterion
	    self._steps = steps
	    self._multiplier = multiplier

	    C_curr = stem_multiplier*C
        # stem 开始conv+bn
	    self.stem = nn.Sequential(
	      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),
	      nn.BatchNorm2d(C_curr)
	    )
 
		C_prev_prev, C_prev, C_curr = C_curr, C_curr, C
		self.cells = nn.ModuleList()
		reduction_prev = False
		# 对每个layers，8个cell
		# 分为normal cell和reduction cell （通道加倍）
		for i in range(layers):
			if i in [layers//3, 2*layers//3]:
				# 共8个cell ，取2-5个cell是作为reduction cell，经过reduction cell，通道加倍
			    C_curr *= 2
			    reduction = True
		 	else:
		    	reduction = False
			cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)
			reduction_prev = reduction
			self.cells += [cell]
			C_prev_prev, C_prev = C_prev, multiplier*C_curr
		
		# cell堆叠之后，后接分类
		self.global_pooling = nn.AdaptiveAvgPool2d(1)
		self.classifier = nn.Linear(C_prev, num_classes)
		
		# 初始化alpha
		self._initialize_alphas()

	# 新建network，copy alpha参数
	def new(self):
		model_new = Network(self._C, self._num_classes, self._layers, self._criterion).cuda()
		for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):
			x.data.copy_(y.data)
		return model_new

	def forward(self, input):
		s0 = s1 = self.stem(input)
		for i, cell in enumerate(self.cells):
			#reduction cell 和normal cell 的 共享参数aplha 不同
	 		if cell.reduction:
	 			# softmax 归一化，14*8，对每一个连接之间的8个op操作进行softmax
	    		weights = F.softmax(self.alphas_reduce, dim=-1)
	 		else:
	    		weights = F.softmax(self.alphas_normal, dim=-1)
	    	# 每个cell之间的连接，s0来自上上个cell输出，s1来自上一个cell的输出
	  		s0, s1 = s1, cell(s0, s1, weights)
		out = self.global_pooling(s1)
		logits = self.classifier(out.view(out.size(0),-1))
		return logits

	def _loss(self, input, target):
		logits = self(input)
		return self._criterion(logits, target) 
	
	# 初始化 alpha
	def _initialize_alphas(self):
		# 14 个连接，4个中间节点 2+3+4+5
		k = sum(1 for i in range(self._steps) for n in range(2+i))
		num_ops = len(PRIMITIVES)
		#14，8
		# normal cell
		# reduction cell
		self.alphas_normal = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)
		self.alphas_reduce = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)
		self._arch_parameters = [
			self.alphas_normal,
			self.alphas_reduce,
			]

	def arch_parameters(self):
		return self._arch_parameters
	
	def genotype(self):
		def _parse(weights):
			gene = []
     		n = 2
     		start = 0
      		for i in range(self._steps):
      	        # 对于每一个中间节点
        		end = start + n
        		# 每个节点对应连接的所有权重 （2，3，4，5）
        		W = weights[start:end].copy()
        		#对于每个节点，根据其与其他节点的连接权重的最大值，来选择最优的2个连接方式（与哪两个节点之间有连接）
        		#注意这里只是选择连接的对应节点，并没有确定对应的连接op，后续确定
        		edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]
        		# 对于最优的两个连接边，分别选择最优的连接op
        		# 这个选择方式，感觉太粗糙了。假设也存在从alpha权重上来看，连接1的第2优的op，比连接2的第1优的op要好。这种操作避免了同一个边的多个op的存在，其实我觉得这种存在也是合理的吧。
        		# 后续有论文对这个选择策略进行改进。如fair-darts，后续blog会讲
        		for j in edges:
					k_best = None
          			for k in range(len(W[j])):
            			if k != PRIMITIVES.index('none'):
              				if k_best is None or W[j][k] > W[j][k_best]:
                				k_best = k
                	# 记录下最好的op，和对应的连接边（与哪个节点相连）
                	# 对于每个节点，选择两个边和对应op，即一个cell有2*4=8个操作，定义死了，不够灵活！
          			gene.append((PRIMITIVES[k_best], j))
        		start = end
        		n += 1
        	return gene

        # 归一化，基于策略选取 每个连接之间最优的操作
		gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())
		gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())
		# 2，6
		concat = range(2+self._steps-self._multiplier, self._steps+2)
		genotype = Genotype(
		normal=gene_normal, normal_concat=concat,
		reduce=gene_reduce, reduce_concat=concat
		)
		return genotype
		
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133
'''
model_search.py
cell的实现，参数共享，分为normal cell 和 reduction cell
经过reduction cell 特征图减半
'''
# 对于 每一条连接
class MixedOp(nn.Module):
	def __init__(self, C, stride):
		super(MixedOp, self).__init__()
		self._ops = nn.ModuleList()
		# 8种op操作
		for primitive in PRIMITIVES:
			# 计算每一种操作
     		op = OPS[primitive](C, stride, False)
     		# 如果操作与pool相关，后接bn
      		if 'pool' in primitive:
        		op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
      		self._ops.append(op)

	def forward(self, x, weights):
		# 每一条连接 所有op，sum。weights每一条连接的每一个op的权重。
    	return sum(w * op(x) for w, op in zip(weights, self._ops))
    	
'''
steps：
multiplier：
C_prev_prev ：上上个cell通道
C_prev：上个cell的通道
reduction：是否是reduction cell
reduction_prev ： 上一个cell是否是 reduction cell
'''
class Cell(nn.Module):
	def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):
    	super(Cell, self).__init__()
    	self.reduction = reduction
    	
		# 对于输入 s0，来自于上上个cell的输出, 通道数C_prev_prev->C
    	if reduction_prev:
    		# 如果这个cell上面的cell是reduction cell
    		# FactorizedReduce，通道数不变C_prev_prev->C，featuremap 减半
    		# 一个conv通道减半C_out // 2，featuremap减半。两个conv，concat
    		# 这个featuremap减半 与 上一个reduction cell 输出的减半的featuremap 规格一样了
      		self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)
    	else:
    		# featuremap大小不变，通道数C_prev->C
      		self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)
      	# 对于输入 s1，来自于上个cell的输出,ReLUConvBN->relu+conv+bn
      	# featuremap大小不变，通道数C_prev->C
		self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)

		self._steps = steps
		self._multiplier = multiplier
		self._ops = nn.ModuleList()
		self._bns = nn.ModuleList()
		# 对于4个中间节点
		for i in range(self._steps): 
			for j in range(2+i):
			    # 如果是reduction cell ，对于一开始的输入 s0，s1到第一个中间节点的连接，stride=2，featuremap实现减半
				stride = 2 if reduction and j < 2 else 1
				#  对于14个连接，MixedOp 8个操作
				op = MixedOp(C, stride)
				self._ops.append(op)

	def forward(self, s0, s1, weights):
		s0 = self.preprocess0(s0)
		s1 = self.preprocess1(s1)
		
		#每个cell之间的连接，s0来自上上个cell输出，s1来自上一个cell的输出
		states = [s0, s1]
		offset = 0
		for i in range(self._steps):
			# 对于每一个节点，计算所有到它的连接的featuremap和。
			#[s0,s1] ops[0](s0,weights[0])+ops[1](s0,weights[1])
			#[s0,s1,sa] ops[2](s0,weights[2])+ops[3](s1,weights[3])+ops[4](sa,weights[4])
			#[s0,s1,sa,sb] (5,6,7,8)
			#[s0,s1,sa,sb,sc] (9,10,11,12,13)
			s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))
			offset += len(states)
			states.append(s)
			#[s0,s1,sa,sb,sc,sd]
		# concat 后面四个节点(sa,sb,sc,sd)的输出，作为整体输出，4*C
		return torch.cat(states[-self._multiplier:], dim=1)
		
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283
```

#### approximate architecture gradient

![img](https://img-blog.csdnimg.cn/20200417155521190.png) 第一步：更新$\alpha$ 第二步：更新$\omega$ architecture search的目标就是通过最小化验证集的loss $L_{val}(w^*,α^*)$来得到最优的$\alpha$。$w^*$是通过最小化训练集loss $L_{train}(w,α^*)$得到的 这是一个bilevel 优化问题。
![优化目标](/workspace/mnt/storage/kanghaidong/video_project/darts/img/优化目标.png)

```python
'''
architect.py
#优化alpha参数

'''
def _concat(xs):
	# 把x view成一行，然后cat成n行
	return torch.cat([x.view(-1) for x in xs]) 
	
#architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)
class Architect(object):

	def __init__(self, model, args):
		self.network_momentum = args.momentum
		self.network_weight_decay = args.weight_decay
		self.model = model
		# 仅优化 arch_parameters
		self.optimizer = torch.optim.Adam(self.model.arch_parameters(),
		lr=args.arch_learning_rate, betas=(0.5, 0.999), weight_decay=args.arch_weight_decay)
   
	def _compute_unrolled_model(self, input, target, eta, network_optimizer):
		# 对omega参数，Ltrain loss
		# theta = theta + v + weight_decay * theta 
		# w − ξ*dwLtrain(w, α)
		loss = self.model._loss(input, target) 
		# n个参数变成n行，需更新的参数theta
		theta = _concat(self.model.parameters()).data 
		try:
			# 增加动量
			moment = _concat(network_optimizer.state[v]['momentum_buffer'] for v in self.model.parameters()).mul_(self.network_momentum)
		except:
			# 不加动量
			moment = torch.zeros_like(theta)
		dtheta = _concat(torch.autograd.grad(loss, self.model.parameters())).data + self.network_weight_decay*theta
		# w − ξ*dwLtrain(w, α) 
		unrolled_model = self._construct_model_from_theta(theta.sub(eta, moment+dtheta))
		return unrolled_model

	def step(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer, unrolled):
		# 清除之前的更新参数值梯度
		self.optimizer.zero_grad()
		if unrolled:
			#用论文的提出的方法
			self._backward_step_unrolled(input_train, target_train, input_valid, target_valid, eta, network_optimizer)
		else:
			# 普通优化，交替优化，仅优化alpha，简单求导
			self._backward_step(input_valid, target_valid)
		self.optimizer.step() # optimizer存了alpha参数的指针

	def _backward_step(self, input_valid, target_valid):
		# 反向传播，计算梯度
		loss = self.model._loss(input_valid, target_valid)
		loss.backward()

	def _backward_step_unrolled(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer):
		# 计算 w' = w − ξ*dwLtrain(w, α)
		# unrolled_model中已经计算完w'
		unrolled_model = self._compute_unrolled_model(input_train, target_train, eta, network_optimizer)
		# 计算 dαLval(w',α) 
		# 对做了一次更新后的w的unrolled_model求验证集的损失，Lval，以用来对α进行更新
		unrolled_loss = unrolled_model._loss(input_valid, target_valid)
		unrolled_loss.backward()
		# dα Lval(w',α)
		dalpha = [v.grad for v in unrolled_model.arch_parameters()]
		# dw'Lval(w',α)
		vector = [v.grad.data for v in unrolled_model.parameters()]
		
		# 计算(dαLtrain(w+,α)-dαLtrain(w-,α))/(2*epsilon) 
		implicit_grads = self._hessian_vector_product(vector, input_train, target_train)
		
    	#  dαLval(w',α)-(dαLtrain(w+,α)-dαLtrain(w-,α))/(2*epsilon)
		for g, ig in zip(dalpha, implicit_grads):
			g.data.sub_(eta, ig.data)
		# 对alpha进行更新
		for v, g in zip(self.model.arch_parameters(), dalpha):
			if v.grad is None:
				v.grad = Variable(g.data)
			else:
				v.grad.data.copy_(g.data)

	def _construct_model_from_theta(self, theta):
		# 新建network，copy alpha参数
		model_new = self.model.new()
		model_dict = self.model.state_dict()
		
		# 按照之前的大小，copy  theta参数
		params, offset = {}, 0
		for k, v in self.model.named_parameters():
			v_length = np.prod(v.size())
			params[k] = theta[offset: offset+v_length].view(v.size())
			offset += v_length
		
		assert offset == len(theta)
		model_dict.update(params)
		model_new.load_state_dict(model_dict)
		# 返回 参数更新为做一次反向传播后的值 的模型
		return model_new.cuda()

  # 计算(dαLtrain(w+,α)-dαLtrain(w-,α))/(2*epsilon)   
  # w+ = w+dw'Lval(w',α)*epsilon 
  # w- = w-dw'Lval(w',α)*epsilon
	def _hessian_vector_product(self, vector, input, target, r=1e-2):
		R = r / _concat(vector).norm()
		# w+ = w+dw'Lval(w',α)*epsilon 
    	for p, v in zip(self.model.parameters(), vector):
      		p.data.add_(R, v)
      	# dαLtrain(w+,α)
    	loss = self.model._loss(input, target)
    	grads_p = torch.autograd.grad(loss, self.model.arch_parameters())
		# w- = w-dw'Lval(w',α)*epsilon
    	for p, v in zip(self.model.parameters(), vector):
      		p.data.sub_(2*R, v)
      	 # dαLtrain(w-,α)
    	loss = self.model._loss(input, target)
    	grads_n = torch.autograd.grad(loss, self.model.arch_parameters())

    	for p, v in zip(self.model.parameters(), vector):
      		p.data.add_(R, v)

    	return [(x-y).div_(2*R) for x, y in zip(grads_p, grads_n)]


123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122
'''
train_search.py
training &&  validation
'''
# training
train_acc, train_obj = train(train_queue, valid_queue, model, architect, criterion, optimizer, lr)
    
# validation
valid_acc, valid_obj = infer(valid_queue, model, criterion)

def train(train_queue, valid_queue, model, architect, criterion, optimizer, lr):
	#...
	#...
	for step, (input, target) in enumerate(train_queue):
    	model.train()
    	input = Variable(input, requires_grad=False).cuda()
    	target = Variable(target, requires_grad=False).cuda()

    	# get a random minibatch from the search queue with replacement
    	input_search, target_search = next(iter(valid_queue))
    	input_search = Variable(input_search, requires_grad=False).cuda()
    	target_search = Variable(target_search, requires_grad=False).cuda()
        
        # 第一步 优化alpha，搜索参数
        # darts是交替优化的.
    	architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)
        # 第二步，优化omega，网络卷积参数
    	optimizer.zero_grad()
    	logits = model(input)
    	loss = criterion(logits, target)
    	loss.backward()
    	nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)
    	optimizer.step()
    	
		prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))
		objs.update(loss.data[0], n)
		top1.update(prec1.data[0], n)
		top5.update(prec5.data[0], n)
	    #...
		#...
	return top1.avg, objs.avg

def infer(valid_queue, model, criterion):
	#...
	#...
	model.eval()
	for step, (input, target) in enumerate(valid_queue):
    	input = Variable(input, volatile=True).cuda()
    	target = Variable(target, volatile=True).cuda()
    	logits = model(input)
    	loss = criterion(logits, target)

   		prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))
    	n = input.size(0)
   	 	objs.update(loss.item(), n)
    	top1.update(prec1.item(), n)
    	top5.update(prec5.item(), n)
		#...
		#...
	return top1.avg, objs.avg
	
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061
```

------

```python
train部分
'''
train.py
#数据准备（cifar10）。
	从cifar10的训练集和验证集中直接取
#网络参数
					搜索网络   训练网络
	init_channels：	  16		36
	layers：		   8		20
	训练网络多一个auxiliary
'''

1234567891011
'''
train.py
子网加载
'''
from model import NetworkCIFAR as Network
genotype = eval("genotypes.%s" % args.arch)
model = Network(args.init_channels, CIFAR_CLASSES, args.layers, args.auxiliary, genotype)

12345678
'''
model.py
子网结构加载
比搜索网络多一个auxiliary，cell也是直接加载权重

'''
class Cell(nn.Module):

  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):
    super(Cell, self).__init__()
    print(C_prev_prev, C_prev, C)

    if reduction_prev:
      self.preprocess0 = FactorizedReduce(C_prev_prev, C)
    else:
      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)
    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)
    
    #  加载genotype
    if reduction:
      op_names, indices = zip(*genotype.reduce)
      concat = genotype.reduce_concat
    else:
      op_names, indices = zip(*genotype.normal)
      concat = genotype.normal_concat
    self._compile(C, op_names, indices, concat, reduction)

  def _compile(self, C, op_names, indices, concat, reduction):
    assert len(op_names) == len(indices)
    self._steps = len(op_names) // 2
    self._concat = concat
    self.multiplier = len(concat)

    self._ops = nn.ModuleList()
    for name, index in zip(op_names, indices):
      stride = 2 if reduction and index < 2 else 1
      op = OPS[name](C, stride, True)
      self._ops += [op]
    self._indices = indices

  def forward(self, s0, s1, drop_prob):
    s0 = self.preprocess0(s0)
    s1 = self.preprocess1(s1)

    states = [s0, s1]
    for i in range(self._steps):
      h1 = states[self._indices[2*i]]
      h2 = states[self._indices[2*i+1]]
      op1 = self._ops[2*i]
      op2 = self._ops[2*i+1]
      h1 = op1(h1)
      h2 = op2(h2)
      if self.training and drop_prob > 0.:
        if not isinstance(op1, Identity):
          h1 = drop_path(h1, drop_prob)
        if not isinstance(op2, Identity):
          h2 = drop_path(h2, drop_prob)
      s = h1 + h2
      states += [s]
    return torch.cat([states[i] for i in self._concat], dim=1)


class AuxiliaryHeadCIFAR(nn.Module):

  def __init__(self, C, num_classes):
    """assuming input size 8x8"""
    super(AuxiliaryHeadCIFAR, self).__init__()
    self.features = nn.Sequential(
      nn.ReLU(inplace=True),
      nn.AvgPool2d(5, stride=3, padding=0, count_include_pad=False), # image size = 2 x 2
      nn.Conv2d(C, 128, 1, bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(inplace=True),
      nn.Conv2d(128, 768, 2, bias=False),
      nn.BatchNorm2d(768),
      nn.ReLU(inplace=True)
    )
    self.classifier = nn.Linear(768, num_classes)

  def forward(self, x):
    x = self.features(x)
    x = self.classifier(x.view(x.size(0),-1))
    return x


'''
网络结构
model = Network(args.init_channels, CIFAR_CLASSES, args.layers, args.auxiliary, genotype)
'''
class NetworkCIFAR(nn.Module):

	def __init__(self, C, num_classes, layers, auxiliary, genotype):
	super(NetworkCIFAR, self).__init__()
	self._layers = layers
	self._auxiliary = auxiliary
	
	stem_multiplier = 3
	C_curr = stem_multiplier*C
	# 主干
	self.stem = nn.Sequential(
	nn.Conv2d(3, C_curr, 3, padding=1, bias=False),
	nn.BatchNorm2d(C_curr)
	)
    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C
    self.cells = nn.ModuleList()
    reduction_prev = False
    for i in range(layers):
      if i in [layers//3, 2*layers//3]:
        C_curr *= 2
        reduction = True
      else:
        reduction = False
      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)
      reduction_prev = reduction
      self.cells += [cell]
      C_prev_prev, C_prev = C_prev, cell.multiplier*C_curr
      
      # 比搜索网络多一个auxiliary
      if i == 2*layers//3:
        C_to_auxiliary = C_prev

	if auxiliary:
		self.auxiliary_head = AuxiliaryHeadCIFAR(C_to_auxiliary, num_classes)
		self.global_pooling = nn.AdaptiveAvgPool2d(1)
		self.classifier = nn.Linear(C_prev, num_classes)

def forward(self, input):
  logits_aux = None
  s0 = s1 = self.stem(input)
  for i, cell in enumerate(self.cells):
    s0, s1 = s1, cell(s0, s1, self.drop_path_prob)
    if i == 2*self._layers//3:
      if self._auxiliary and self.training:
        logits_aux = self.auxiliary_head(s1)
  out = self.global_pooling(s1)
  logits = self.classifier(out.view(out.size(0),-1))
  return logits, logits_aux



123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140
'''
train.py
子网训练
'''
for epoch in range(args.epochs):
    scheduler.step()
    # ...
    model.drop_path_prob = args.drop_path_prob * epoch / args.epochs
    train_acc, train_obj = train(train_queue, model, criterion, optimizer)
    valid_acc, valid_obj = infer(valid_queue, model, criterion)
   # ...

def train(train_queue, model, criterion, optimizer):
	objs = utils.AvgrageMeter()
	top1 = utils.AvgrageMeter()
	top5 = utils.AvgrageMeter()
	model.train()

	for step, (input, target) in enumerate(train_queue):
		input = Variable(input).cuda()
	    target = Variable(target).cuda()
	    optimizer.zero_grad()
	    # logits_aux
	    logits, logits_aux = model(input)
	    loss = criterion(logits, target)
	    # 采用附加结构loss*权重
    	if args.auxiliary:
			loss_aux = criterion(logits_aux, target)
			loss += args.auxiliary_weight*loss_aux
    	loss.backward()
		nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
		optimizer.step()

		prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))
		n = input.size(0)
		objs.update(loss.item(), n)
		top1.update(prec1.item(), n)
		top5.update(prec5.item(), n)
	return top1.avg, objs.avg


def infer(valid_queue, model, criterion):
  objs = utils.AvgrageMeter()
  top1 = utils.AvgrageMeter()
  top5 = utils.AvgrageMeter()
  model.eval()

  for step, (input, target) in enumerate(valid_queue):
    input = Variable(input, volatile=True).cuda()
    target = Variable(target, volatile=True).cuda()

    logits, _ = model(input)
    loss = criterion(logits, target)

    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))
    n = input.size(0)
    objs.update(loss.item(), n)
    top1.update(prec1.item(), n)
    top5.update(prec5.item(), n)

    if step % args.report_freq == 0:
      logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)

  return top1.avg, objs.avg
  
```